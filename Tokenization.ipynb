{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Stemming</h3>\n",
    "\n",
    "  Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes of to the roots of words known as lemma. Stemming is important in natural language understading(NLU) and Natural Language Processing(NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./package.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus =\"\"\"Hello welcome to Krish Naik's NLP Tutorials.\n",
    "Please do watch the entire course! to become expert in NLP\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Hello welcome to Krish Naik's NLP Tutorials.\", 'Please do watch the entire course!', 'to become expert in NLP']\n",
      "Hello welcome to Krish Naik's NLP Tutorials.\n",
      "Please do watch the entire course!\n",
      "to become expert in NLP\n"
     ]
    }
   ],
   "source": [
    "### Tokenizer\n",
    "### paragragh --> sentence\n",
    "documents = sent_tokenize(corpus)\n",
    "print(documents)\n",
    "for sentence in documents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'welcome', 'to', 'Krish', 'Naik', \"'s\", 'NLP', 'Tutorials', '.', 'Please', 'do', 'watch', 'the', 'entire', 'course', '!', 'to', 'become', 'expert', 'in', 'NLP']\n"
     ]
    }
   ],
   "source": [
    "## paragraph --> words\n",
    "## sentence --> words\n",
    "word = word_tokenize(corpus)\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'welcome', 'to', 'Krish', 'Naik', \"'s\", 'NLP', 'Tutorials', '.']\n",
      "['Please', 'do', 'watch', 'the', 'entire', 'course', '!']\n",
      "['to', 'become', 'expert', 'in', 'NLP']\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'welcome', 'to', 'Krish', 'Naik', \"'\", 's', 'NLP', 'Tutorials', '.']\n",
      "['Please', 'do', 'watch', 'the', 'entire', 'course', '!']\n",
      "['to', 'become', 'expert', 'in', 'NLP']\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(wordpunct_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'welcome', 'to', 'Krish', 'Naik', \"'s\", 'NLP', 'Tutorials.', 'Please', 'do', 'watch', 'the', 'entire', 'course', '!', 'to', 'become', 'expert', 'in', 'NLP']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer = tokenizer.tokenize(corpus)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./package.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classificatio Problem : To get words for marking email as spam or ham\n",
    "words = ['eating','eat','eaten','writing','writes','history','programming','programs','sweet','congratulations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating eat\n",
      "eat eat\n",
      "eaten eaten\n",
      "writing write\n",
      "writes write\n",
      "history histori\n",
      "programming program\n",
      "programs program\n",
      "sweet sweet\n",
      "congratulations congratul\n"
     ]
    }
   ],
   "source": [
    "stemming = PorterStemmer()\n",
    "for word in words:\n",
    "    print(word,stemming.stem(word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RegexStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating eat\n",
      "eat eat\n",
      "eaten eaten\n",
      "writing writ\n",
      "writes write\n",
      "history history\n",
      "programming programm\n",
      "programs program\n",
      "sweet weet\n",
      "congratulations congratulation\n"
     ]
    }
   ],
   "source": [
    "## RegexStemmer does stemming along with Regular Expression\n",
    "reg_stemming = RegexpStemmer(r'ing$|s\\b|\\bs',min=4)\n",
    "for word in words:\n",
    "    print(word,reg_stemming.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating eat\n",
      "eat eat\n",
      "eaten eaten\n",
      "writing write\n",
      "writes write\n",
      "history histori\n",
      "programming program\n",
      "programs program\n",
      "sweet sweet\n",
      "congratulations congratul\n"
     ]
    }
   ],
   "source": [
    "snow_stemming = SnowballStemmer(language='english')\n",
    "for word in words:\n",
    "    print(word,snow_stemming.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
